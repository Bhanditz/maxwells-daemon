% Sample LaTeX file for creating a paper in the Morgan Kaufmannn two
% column, 8 1/2 by 11 inch proceedings format.

\documentclass[]{article}
\usepackage{uai2015stylefiles/proceed2e}

% Set the typeface to Times Roman
\usepackage{times}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{color}
\usepackage{algorithm}
\usepackage{todonotes}

\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\hypersetup{ %
    pdftitle={},
    pdfauthor={},
    pdfsubject={},
    pdfkeywords={},
    pdfborder=0 0 0,
    pdfpagemode=UseNone,
    colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue,
    pdfview=FitH}

% Math typesetting.
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vX}{\mathbf{X}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vr}{\mathbf{r}}
\newcommand{\vg}{\mathbf{g}}
\newcommand{\vI}{\mathbf{I}}
\newcommand{\vzero}{\bf{0}}
\newcommand{\ones}[1]{\mat{1}_{#1}}
\newcommand{\eye}[1]{\mat{E}_{#1}}
\newcommand{\tra}{^{\mathsf{T}}}
\newcommand{\vect}[1]{{\bf{#1}}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\npderiv}[2]{\nicefrac{\partial #1}{\partial #2}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\expect}{\mathbb{E}}
\newcommand{\expectargs}[2]{\mathbb{E}_{#1} \left[ {#2} \right]}
\newcommand{\var}{\mathbb{V}}
\newcommand{\varL}{\mathcal{L}}
\def\iid{i.i.d.\ }
\def\simiid{\overset{\mbox{\tiny iid}}{\sim}}
\newcommand{\defeq}{\mathrel{:\mkern-0.25mu=}}
\newcommand{\Normal}{\mathcal{N}}
\newcommand{\Nt}[3]{\mathcal{N}\!\left(#1 \middle| #2,#3\right)}
\newcommand{\N}[2]{\mathcal{N}\!\left(#1,#2\right)}
\DeclareMathOperator{\KLop}{KL}
\newcommand{\KL}[2]{\KLop \left(#1 \middle \| #2 \right)}

% Symbol definitions.
\newcommand{\distinit}{q_0(\params, \vv)}
\newcommand{\data}{\vx}

%% \newcommand{\params}{\vx}
\newcommand{\params}{\mathbf{\theta}}
\newcommand{\trans}{T}
\newcommand{\paramsrv}{\vX}  % Random variable.
\newcommand{\numsteps}{T}
\newcommand{\decay}{\gamma}
\newcommand{\decays}{{\boldsymbol{\decay}}}
\newcommand{\stepsize}{\alpha}
\newcommand{\stepsizes}{{\boldsymbol{\stepsize}}}
\newcommand{\gradparams}{\nabla L(\params_t, t)}
\DeclareMathOperator{\SGD}{SGD}
\newcommand{\entropy}{H}
\newcommand{\pun}{{\tilde p}}
\newcommand{\jointdist}{p(\params , \data)}
\newcommand{\posterior}{p(\params | \data)}
\newcommand{\subjointdist}[2]{p_{#1}(\params_{#2} , \data)}
\newcommand{\subjointdistminibatch}[1]{\tilde{p}(\params_{#1} , \data)}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\bigo}[1]{\mathcal{O}\left(#1\right)}
\newcommand{\trace}[1]{\text{Tr}\left[#1\right]}

%\title{Maxwell's D\ae mon: Stochastic Gradient Nonparametric Variational Inference}
%\title{Entropic Descent: Stochastic Gradient Nonparametric Variational Inference}
%\title{Stochastic Gradient Descent is Nonparametric Variational Inference}
\title{Early Stopping is Nonparametric Variational Inference}

\author{} % LEAVE BLANK FOR ORIGINAL SUBMISSION.
          % UAI  reviewing is double-blind.

% The author names and affiliations should appear only in the accepted paper.
%
%\author{ {\bf Harry Q.~Bovik\thanks{Footnote for author to give an
%alternate address.}} \\
%Computer Science Dept. \\
%Cranberry University\\
%Pittsburgh, PA 15213 \\
%\And
%{\bf Coauthor}  \\
%Affiliation          \\
%Address \\
%\And
%{\bf Coauthor}   \\
%Affiliation \\
%Address    \\
%(if needed)\\
%}

\begin{document}

\maketitle

\begin{abstract}
We show that unconverged stochastic gradient descent and other popular optimization methods can be interpreted as procedures that generate samples from a nonparametric variational approximate posterior distribution.
This distribution is implicitly defined as the transformation of an initial distribution by a sequence of optimization updates into a final approximation.
Each step of the optimization results in an intermediate distribution and by keeping track of the changes in entropy over this sequence of distributions, we can form an unbiased estimator of a lower bound on the log marginal likelihood.
We can then maximize this bound with respect to the models' hyperparameters instead of using a separate validation set.
This Bayesian interpretation of SGD suggests improved overfitting-resistant optimization procedures and gives a theoretical foundation for popular tricks such as early stopping and ensembling.
Moreover, this procedure hints at a middle ground between point estimates arising from MAP inference, and fully-factored variational approximations.
\end{abstract}

\section{Introduction}

In much of machine learning, the central computational challenge is optimization; we try to maximize some training set objective with respect to a set of model parameters.
If we treat the training objective as a (negative) log-posterior, this amounts to searching for a maximum \emph{a posteriori} (MAP) solution.
Paradoxically, over-zealous optimization can yield worse test set results than incomplete optimization due to the phenomenon of \emph{over-training}.
A popular remedy to over-training is to invoke ``early stopping'' in which optimization is halted based on the continually monitored performance of the parameters on a separate validation set.
However, early stopping is both theoretically unsatisfying and incoherent from a research perspective: how can one rationally design better optimization methods if the goal is to achieve something ``powerful but not \emph{too} powerful''?
A related trick is to ensemble the results from multiple optimization runs from different starting positions.
Similarly, this must rely on imperfect optimization, since otherwise all optimization runs would reach the same optimum.

\begin{figure}[t]
\vskip 0.2in
\begin{center}
\includegraphics[width=\columnwidth]{../experiments/2015_03_02_funnel/2/dists.pdf}
\caption{A series of variational distributions implicitly defined by
  stochastic gradient descent on the log-likelihood.
  The initial distribution from which parameters are drawn (red) doesn't match the posterior (black).
  Intermediate distributions (green and blue) are implicitly defined by mapping each initial point through many iterations of gradient descent.
  These distributions don't have fixed parametric shapes, but depend on the shape of the entire posterior.}
\label{fig:cartoon}
\end{center}
\end{figure}

We propose an interpretation of incomplete optimization in terms of variational Bayesian inference.
Our starting point is a Bayesian posterior distribution for a potentially complicated model, in which there is an empirical loss that can be interpreted as a negative log likelihood and regularizers that have interpretations as priors.
One might proceed with MAP inference in such a setting and perform an optimization to find the best parameters.
The main idea of this paper is that such an optimization procedure, initialized according to some distribution that can be chosen freely, generates a sequence of distributions that are implicitly defined by the action of the optimization update rule on the previous distribution.
We can treat these distributions as variational approximations to the true posterior distribution.
A single optimization run for~$N$ iterations represents a draw from the~$N$th such distribution in the sequence.
For many popular optimization methods this sequence of distributions can be used to form an unbiased estimate of a lower bound on the log marginal likelihood.

With this interpretation, the number of optimization iterations can be seen as a variational parameter, one that trades off fitting the data well against maintaining a broad (high entropy) distribution.
Early stopping amounts to optimizing the variational lower bound (or an approximation based on a validation set) with respect to this variational parameter.
Similarly, since each optimization trajectory from a random initialization represents a sample from the variational distribution, ensembling random restarts can be viewed as building a sample-based approximation of (an approximation to) the posterior.

To establish whether this viewpoint is helpful in practice we ask (a)~whether we can estimate the marginal likelihood, and (b)~whether the family of variational distributions implied by the optimization rule yields a reasonably good approximation to the true posterior.
We tackle the first question in sections \todo{XX} and \todo{YY} and show that many popular optimization procedures yield tractable estimates of the marginal likelihood.
The second question is an empirical one, and we give some experimental evidence in both directions.

\subsection{Contributions}
\begin{itemize}
\item We give a new interpretation to optimization algorithms as samplers from a variational distribution that adapts to the true posterior.
\item We give a scalable estimate the entropy of the implicit variational distributions, allowing us to estimate a lower bound on the marginal likelihood even on problems with hundreds of thousands of parameters.
\item We investigate the performance of these estimators empirically.
\end{itemize}

\section{Incomplete optimization as variational inference}

Variational inference \todo{cite Hinton and also Wainwright and Jordan, possibly Ghahramani et al} aims to approximate an intractable posterior distribution, $\posterior$, with another distribution, $q(\params)$.
Ideally, the distribution~$q(\params)$ would belong to a simpler and tractable class, such as factored distributions or a parametric family.
The usual approach is to quantify the quality of the approximation using the Kullback-Leibler (KL) divergence from $q(\params)$ to $\jointdist$.
Such a measure also provides a lower bound on the marginal likelihood of the original model; rearranging Bayes' rule gives the familiar inequality:
%
\begin{align}
\log p(\data)
& \geq - \underbrace{\expectargs{q(\params)}{ -\log \jointdist }}_{\textnormal{\normalsize Energy $E[q]$}}
         \underbrace{- \expectargs{q(\params)}{\log  q(\params)}}_{\textnormal{\normalsize Entropy $S[q]$}} \nonumber \\
& := \varL[q] \label{eq:varbound}
\end{align}
%
Maximizing $\varL[q]$, the variational lower bound on the marginal likelihood, with respect to $q$ minimizes $\KL{q(\params)}{\posterior}$, the KL divergence from $q$ to the true posterior, giving the closest approximation available within the variational family.
A convenient side effect is that we also get a lower bound on $p(\data)$, which can be used for model selection.

To perform variational inference, we require a family of distributions over which to maximize $\varL[q]$. 
Consider a general procedure to minimize the energy~$-\jointdist$ with respect to~${\params \in \reals^D}$.
The parameters~$\params$ are initialized according to some distribution~$q_0(\params)$ and updated at each iteration according to a transition operation~${\trans : \reals^D \rightarrow \reals^D}$:
%
\begin{align}
\params_0 &\sim q_0(\params) \nonumber \\
\params_{t + 1} &= \trans(\params_t), \nonumber
\end{align}
%
Our variational family consists of the sequence of distributions~$q_0, q_1, q_2, \ldots$ implied by these transformations of~$q_0$, where~$q_t(\params)$ is the distribution over~$\params_t$ generated by the above procedure.

As shown in (\ref{eq:varbound}), $\varL$ consists of an `energy' term and an `entropy' term.
The energy term measures how well~$q$ fits the data and the entropy term encourages the probability mass of~$q$ to spread out, preventing overfitting. \todo{not totally comfortable with the entropy term being about overfitting exactly}
As optimization of~$\params$ proceeds from its~$q_0$-distributed starting point, we can examine how~$\varL$ changes
The negative energy term grows, since the goal of the optimization is to reduce the energy, while the entropy term shrinks, since the optimization converges over time.
Optimization thus generates a sequence of distributions that range from underfitting to overfitting, and the variational lower bound captures this tradeoff.

We cannot evaluate $\varL[q_t]$ exactly, but we can obtain an unbiased estimator.
Sampling~$\params_0$ from~$q_0$ and then applying the transition operator~$t$ times produces an exact sample~$\params_0$ from~$q_t$, by definition.
Since $\params_t$ is an exact sample from $q_t(\theta)$, $\log\subjointdist{}{t}$ is an unbiased estimator of the energy term of (\ref{eq:varbound}).
The entropy term is trickier, since we do not have access to the density $q(\params)$ directly.
However, if we know the entropy of the initial distribution, $S[q_0(\params)]$, then we can estimate $S[q_0(\params)]$ by looking at the change in entropy at each iteration, calculated by considering the change of variables formula for a probability density function.
To compute how the volume shrinks or expands due to this change of variables, we require access to the Jacobian of the optimizer's transition operator, $J(\params)$:
%
\begin{align}
S[q_{t+1}] - S[q_t] =
  \expectargs{q_t(\params_t)}{\log
    \left| J(\params_t) \right|} \,.
  \nonumber
\end{align}
%
Note that this analysis assumes that the mapping $T$ is bijective.
Combining these terms, we have an unbiased estimator of $\varL$ at iteration $T$:
\begin{align}
\varL[q_T] \approx
  \underbrace{\log \subjointdist{}{T}}_{\textnormal{\normalsize Energy}} +
  \underbrace{\sum_{t=0}^{T-1} \log \left| J(\params_t) \right| + S[q_0]}_{\textnormal{\normalsize Entropy}} \,.
  \nonumber
\end{align}



\subsection{Stochastic Gradient Descent}

Stochastic gradient descent is a popular and effective optimization procedure with the following update rule:
\begin{align}
\params_{t+1} &=
  \params_t - \stepsize \nabla_{\params}
  \left[-\log \subjointdist{t}{t} \right]\,,\nonumber
\end{align}
where the $\subjointdist{t}{}$ are a sequence of unbiased estimators of~$\jointdist$.\todo{well, unbiased estimators of the gradient is what we want.  also, it doesn't really have to be unbiased}
Taking the Jacobian of this update rule, we get the following unbiased estimator for the change in entropy:
\begin{align}
S[q_{t+1}] - S[q_t] \approx \log \left| I - \stepsize H_t(\params_t)
\right| \label{eq:exact hessian}
\end{align}
where $H_t$ is the Hessian of $-\log\subjointdist{t}{}$ with respect to~$\params$.
Note that the Hessian does not need to be positive definite or even non-singular.
If some directions in $\params$ have negative curvature, the crest of a hill, it just means that optimization near there spreads out probability mass, increasing the entropy.

There are, however, restrictions on $\stepsize$.
If ${\stepsize\lambda_i = 1}$, for any $i$, where $\lambda_i$ are the eigenvalues of $H_t$, then the change in entropy will be undefined (infinitely negative).
Physically, this corresponds to a Newton-like update where all points collapse to the optimum in a single step giving a distribution with zero variance in a particular direction.
However, gradient descent is unstable anyway if ${\stepsize\lambda_{\text{max}} > 2}$, where~$\lambda_{\text{max}}$ is the largest eigenvalue of~$H_t$.
So if we
choose a small enough step size so that $\stepsize\lambda_{\text{max}} < 1$,
this situation should not be a problem.

\subsection{Estimating the Jacobian in high dimensions}

The expression for the change in entropy given by (\ref{eq:exact hessian}) is impractical for large-scale problems since it requires an~$\bigo{D^3}$ determinant computation.
Fortunately, we can make a good approximation using just one or two Hessian-vector products, which can usually be performed in~$\bigo{D}$ time using reverse-mode differentiation \todo{cite Perlmutter}.

The idea is that since~$\stepsize\lambda_{\text{max}}$ is small, the Jacobian is actually just a small perturbation to the identity and we can approximate its determinant using traces as follows:
\begin{align}
\log \left| I - \stepsize H \right|
& =    \sum_{i=0}^D \log\left(1 - \stepsize\lambda_i\right) \nonumber \\
& \geq \sum_{i=0}^D \left[- \stepsize\lambda_i 
                        - (\stepsize\lambda_i)^2 \right] \label{eq:logbound} \\
& = - \stepsize \trace{H} - \stepsize^2 \trace{HH}\,.
\end{align}
The bound in (\ref{eq:logbound}) is just a second order Taylor expansion of~$\log(1 - x)$ about~${x = 0}$ and is valid if ${\stepsize\lambda_i < 0.7}$. \todo{little o terms here for clarity?}
\todo{Mention closeness of bound.}
The trace of the Hessian can be estimated using inner products of random vectors:
\begin{align}
\trace{H} = \expectargs{}{\vr^TH\vr}, \qquad \vr \sim \N{0}{I}\,.
\label{eq:approx-log-det}
\end{align}
\todo{wouldn't mind a ref for that}
A common problem, similar to the one we tackle here, is to compute the log of the determinant of the Hessian itself.
This arises, for example, in making the Laplace approximation to the posterior \todo{cite Mackay}.
This is a much harder problem since the Hessian can be arbitrarily ill-conditioned, unlike our small Hessian-based perturbation to the identity.

\begin{algorithm}[t]
   \caption{gradient descent with marginal likelihood estimate}
   \label{alg:sgd-with-estimate}
\begin{algorithmic}[1]
	\State {\bfseries input:}
	Weight initialization scale $\sigma_0$, step size $\stepsize$, \\	
	negative log likelihood $L(\params, t)$	
	\State initialize $\params_1 \sim \N{0}{\sigma_0 \vI_D}$
	\State $\entropy = \frac{D}{2} (1 + \log 2 \pi) + D \log\sigma_0$
	\For{$t=1$ {\bfseries to} $T$}
%		\State $\vg = \gradparams$             \Comment{Evaluate gradient}
%		\State $\vr \sim \N{0}{\vI_D}$         \Comment{Draw random direction}
%		\State $\vr\tra \left[ -2 \vr + 3 \left(  R1 - R2 \right) \right]$      \Comment{Estimate determinant}
		\State $\entropy = \entropy - \log \det | \vI - \stepsize H_t |$  \Comment{Update entropy} \label{step:entropy-update}
		\State $\params_{t+1} = \params_t + \stepsize \gradparams$  \Comment{Update position}		
   \EndFor
   \State \textbf{output} sample $\params_T$, estimate of entropy $\entropy$
\end{algorithmic}
\end{algorithm}
%
Algorithm~\ref{alg:sgd-with-estimate} combines these steps into an algorithm that tracks the approximate entropy during optimization.
In high dimensions, step~\ref{step:entropy-update} should be replaced with equation~\eqref{eq:approx-log-det}.



\section{Experiments}

(Planning to test out how well it performs in practice. Not claiming it's the
best thing ever. Just scientifically interested in how good of an approximation
it is.)

In general, what do we most care about in this section?
\begin{itemize}
\item Quality of marginal likelihood estiamte
\item Quality of approximating distribution
\item Comparison between marginal likelihood estimate and held-out likelihood
\item Slackness of bound due to imperfect guessing of distribution when using
  noisy/mcmc version
\item Quality of Hessian randomized trace estimate
\item Noisiness of stochastic estimates
\end{itemize}

\subsection{Boston Housing Data}


\begin{figure}[t]
%\vskip 0.2in
\begin{center}
\includegraphics[width=\columnwidth]{../experiments/2015_03_01_housing/1/marglik}
\vskip -0.1in
\caption{\emph{Top}: Test-set error on the Boston housing dataset.
\emph{Bottom}: Marginal likelihood estimate.
The marginal likelihood, estimated during training using only the training set, has a maximum at an iteration near that where the test error is best.}
\label{fig:housing}
\end{center}
\end{figure}


Figure \ref{fig:housing} shows overfitting and show marginal likelihood peaks at a similar place to the peak of held-out log-likelihood.
[TODO: Use SGD with exact Hessian and no noise.]


\subsection{MNIST}
(Show that noiseless version doesn't perform particularly well. Noise does a
good job with the overfitting problem. But does can we capture the marginal
likelood well? Do we need to do AIS to check?)

\begin{figure}
%\vskip 0.2in
\begin{center}
\includegraphics[width=\columnwidth]{../experiments/2015_02_27_first_entropic_sgd/6_ensemble/ensemble}
\vskip -0.1in
\caption{\emph{Top}: Marginal likelihood estimate.
\emph{Bottom}: Test-set error on the MNIST dataset.}
\label{fig:ensemble}
\end{center}
\end{figure}

Figure \ref{fig:ensemble} shows...

\section{Related work}

RMSprop~\cite{Tieleman2012} or Adam~\citep{Adam14}

\paragraph{Reversible learning} \citet{MacDuvAda2015hyper}. Optimization is an
intrinsically information-destroying process since a (good) optimization
procedure maps any initial starting point to one or a few final optima. We can
quantify the loss of information by asking how many bits must be stored in order
to reverse the optimization. We can think of this as average the number of bits
`learned' during the optimization. From this perspective, stopping before optimization
converges can be seen as a way to limit the number of bits we try to learn,
since we don't expect to be able to learn more than a finite number of bits; cite reversible
learning paper. Thus early stopping is one of many ways to reduce the hypothesis
space. (cite MDL? Statistical learning theory?)

\paragraph{MCMC for variational inference}
Our method can be seen as a special case of \citet{Bridging14}.


\paragraph{Bayesian neural networks}
Variational inference in Bayesian neural-network models such as deep Gaussian processes~\citep{deepGPVar14}.
\citet{kingma2014efficient} show how neural networks having unknown weights can be reformulated as neural networks having known weights but stochastic hidden units, and exploit this connection to preform efficient gradient-based inference in Bayesian neural networks.


\paragraph{Black-box stochastic variational inference}
\citet{alp2014blackbox} introduce a general scheme for variational inference using only the gradients of the log-likelihood of a model.
However, they constrain their variational approximation to be Gaussian, as opposed to our free-form variational distribution.

\paragraph{Adaptive learning rates for SGD}
\citet{courbariaux2014low} give a method based on Hessian-vector products to adapt learning rates for stochastic gradient descent without momentum.


\paragraph{Langevin Dynamics}
\citet{welling2011bayesian} introduced stochastic gradient Langevin dynamics for doing inference with minibatches.
\citet{ma2013estimating} use Langevin dynamics and a floating temperature to estimate partition functions of graphical models.
\citet{vollmer2015non} analyze the asymptotics of Langevin dynamics.

\paragraph{Annealed HMC}
\citet{sohl2012hamiltonian} used HMC including an accept/reject step for annealed importance sampling to estimate partition functions.

\paragraph{HMC with fewer rejections}
\citet{sohl2014hamiltonian} introduce a variant of HMC that requires fewer rejections by extending trajectories in some cases where a rejection would otherwise occur, giving a sampler that does not obey detailed balance but still samples from the correct stationary distribution.

\paragraph{MCMC with mini batches}
[TODO: more citations go here]
\citet{betancourt2015fundamental} argues that minibatches are fundamentally incompatible with HMC.


\section{Future Work and Extensions}

We could derive a variational inference algorithm based on RMSProp, and a corresponding variant of Hamiltonian Monte Carlo.

Hyperparameters typically come in two forms:
Regularization parameters and training parameters.
Optimizing marginal likelihood rather than training loss lets us set regularization parameters during training without using a validation set.
The marginal likelihood estimate lets us optimize the variational parameters (training hyperparameters) in an outer loop.
However, optimizing more than a few of these is difficult without gradients.
We could gain access to exact gradients of the variational lower bound with respect to all variational parameters by simply using reverse-mode differentiation.
\citet{MacDuvAda2015hyper} showed that this can be done in a memory-efficient way for momentum-based learning procedures.
Combining these two procedures would allow one to set all hyperparameters using gradient-based methods without the need for a validation set.

%\subsection{Acknowledgements}
%We thank Roger Grosse for helpful discussions.


[TODO: Cite Nando's variational MCMC paper]


\bibliography{references.bib}
\bibliographystyle{uai2015stylefiles/icml2015}

\end{document}

% ==============================================================================




\subsection{Adding noise to increase entropy}
(Adding noise is the obvious way to increase entropy. If noise variance is
exactly double step size then this is just stochastic langevin dynamic (cite
Welling and Teh) and if we're doing MCMC then this is just like bridging the gap
paper. But the hard part is now estimating entropy. It requires an estimate of q
itself but we don't have access to q directly. Luckily, any q will do, it will
just be a looser bound if we get it wrong. The approach of Welling et al is
to make a parametric form which is learned. We propose a simpler approach. We
expect it to eventually converge to the true posterior, and we know the initial
distribution. So we approximate as it goes along with a convex combination of
these two log probs. Annealing so that we're actaully targeting these as we go
along is another possiblity.)

(Maybe mention that treating sgd as a deterministic procedure is a bit
disingenuous: it might be formally deterministic but the selection of
minibatches is certainly arbitrary. Perhaps you could get better performance by
formalizing the randomness and accounting for it.)



We define a nonparametric variational distribution implicitly as the output of a
random initialization run through $T$ iterations of stochastic gradient descent
(SGD):

%
\begin{align}
\params_1, \vv_1 & \sim \distinit \\
q^{\SGD}_T(\params, \vv) & = \SGD(\stepsizes, \decays, \params_1, \vv_1, T)
\end{align}
%

\subsection{Leapfrog Dynamics}

In the limit of small step sizes $\stepsize$, leapfrog dynamics exactly preserve the energy and entropy of the system, and therefore the marginal likelihood of the implicit variational distribution remains the same.

No matter the step size, leapfrog dynamics always exactly preserve entropy.
The only time entropy is added or removed from the system is when the momentum is resampled.
The difference in this entropy is given by:
%
\begin{align}
\Delta H & = H_t' - H_t \\
 & = - \expectargs{q_(\vx)}{\expect_{q'(\vv|\vx)} \log q'(\vv|\vx) - \expect_{q(\vv|\vx)} \log q(\vv|\vx)} \nonumber \\
 & = - \expect{q_(\vx)} \Bigg[ \expect_{q'(\vv|\vx)} \log q'(\vv|\vx) - \expect_{q(\vv|\vx)} \log r(\vv|\vx) \nonumber \\ 
 & \quad - \underbrace{\expect_{q(\vv|\vx)} \log \frac{ q(\vv|\vx)}{r(\vv|\vx)}}_{\textnormal{\scriptsize $\KL{q(\vv|\vx)}{r(\vv|\vx)}$}} \Bigg] \label{eq:r slack}
\end{align}
%

In the case of SGD with momentum, entropy is lost only when drag is applied.
If the momentum is decayed by $\vv_{t+1} = \decay_t \vv_t$, then the amount of entropy lost is exactly $\log \decay$.

\begin{figure}[t]
%\vskip 0.2in
\begin{center}
\includegraphics[width=\columnwidth]{../experiments/2015_02_23_gaussian_phase_diagrams/1/phase_diagram}
\caption{Phase diagram over time.}
\label{fig:phase}
\end{center}
\end{figure}


\begin{algorithm}
   \caption{Entropic Descent}
   \label{alg:entropic-descent}
\begin{algorithmic}[1]
	\State {\bfseries input:}
	Weight initialization scale $\sigma_0$, \\
	velocity randomization level $\decay$, \\
	annealing schedule $\stepsizes$, \\
	time step $\epsilon$, \\
	highest eigenvalue $\lambda$, \\
	negative log likelihood $L(\params, t)$
	\State initialize $\vv_1 \sim \N{0}{\vI_D}$
	\State initialize $\params_1 \sim \N{0}{\sigma_0 \vI_D}$
	\State $\entropy = \frac{D}{2} (1 + \log 2 \pi) + D \log\sigma_0 + \frac{1}{2} (D - |\vv|_2^2)$
	\For{$t=1$ {\bfseries to} $T$}
		\State $\entropy = \entropy + \frac{1}{2} |\vv|_2^2$ \Comment{Update entropy}
		\State $d =   \stepsizes_t \lambda     + (1 - \stepsizes_t) \sigma_0^2$ \Comment{Anneal stepsize}
		\State $\vg = \stepsizes_t \gradparams + (1 - \stepsizes_t) \frac{\vx}{\sigma_0^2}$ \Comment{Anneal gradient}
		\State $\vv_{t+1} = \vv_t + d \times \vg$ \Comment{Update velocity}
		\State $\params_{t+1} = \params_t + d \times \vv$  \Comment{Update position}
		\State $\entropy = \entropy - \frac{1}{2} |\vv|_2^2$  \Comment{Downdate entropy}
		\State $\vr \sim \N{0}{\vI_D}$
		\State $d\vv = \sqrt{1 - \decay^2} d\vv + \decay \vr$ \Comment{Perturb velocity}
   \EndFor
   \State \textbf{output} sample $\params_T$, estimate of entropy $\entropy$
\end{algorithmic}
\end{algorithm}

$\lambda$ is the square root of the highest eigenvalue of the Hessian of the log-posterior at the mode.
$\decay$ controls the amount velocities randomize each iteration.

Since we can sample exactly from $q$, we can approximate the first (``energy'') term of $L$ using one or more Monte Carlo samples.

The variational parameters of $q^{\SGD}$ are the learning rate schedule $\stepsizes$, the momentum decay schedule $\decays$ and the initial distribution $\distinit$.
Perhaps for now we'll set $\distinit$ to be the prior, although it could really be anything.

\subsection{The need for Annealing}
If we didn't anneal, the KL term in equation \ref{eq:r slack} wouldn't go to zero even as the step-size approached zero.

\subsection{But what about detailed balance?}



\begin{figure*}[t]
%\vskip 0.2in
\begin{center}
\includegraphics[width=0.18\linewidth]{../experiments/2015_02_19_caibration/5_figure/seqs/densities_0}
\includegraphics[width=0.18\linewidth]{../experiments/2015_02_19_caibration/5_figure/seqs/densities_200}
\includegraphics[width=0.18\linewidth]{../experiments/2015_02_19_caibration/5_figure/seqs/densities_400}
\includegraphics[width=0.18\linewidth]{../experiments/2015_02_19_caibration/5_figure/seqs/densities_600}
\includegraphics[width=0.18\linewidth]{../experiments/2015_02_19_caibration/5_figure/seqs/densities_800}
\caption{Left to right: Phase diagrams over time.}
\label{fig:dynamics}
\end{center}
\end{figure*}



\section{Online gradient updates of learning rate and momentum}

A simple procedure to sample from a variational posterior could look like this:
Start with $\stepsize_1 = 1$, $\decay_1 = 1$.
At each iteration of SGD, update each in the direction that would improve the bound at computed at the current step,~$L_{t+1}$:
%
\begin{align}
\vv_{t+1}     & = \decay_t \vv_t - g(\params_t) \\
\params_{t+1} & = \params_t + \stepsize_t \vv_{t+1} \\
\stepsize_{t+1} & = \stepsize_t + s_\stepsize \pderiv{L_t}{\stepsize_t} \\
\decay_{t+1} & = \decay_t + s_\decay \pderiv{L_t}{\decay_t}
\end{align}
%
where $s_\stepsize$ and $s_\decay$ are `meta' step sizes.

Which direction do these gradients point?
For $\stepsize$,
%
\begin{align}
\pderiv{L_t}{\stepsize_t} = g(\params_{t+1}) \vv_{t+1}
\end{align}
%
which means that the learning rate will increase as long as the gradient is pointing in the same direction as the velocity.
It also has the effect of going at the same speed when the gradient becomes small.
This is what we want:  If learning stopped when the gradient was zero, then the optimizer would converge.

For $\decay$, the gradient has the form:
%
\begin{align}
\pderiv{L_t}{\decay_t} = - g(\params_{t+1}) \vv_{t} \stepsize_t - \log \decay_t
\end{align}
%
The first term is exactly the same as our heuristic, scaled by $\stepsize$!
It will increase the velocity if the gradient points in the opposite direction from the velocity.

The second term has a damping effect:  If the gradient, velocity, or stepsize are all small, then this term will try to set the momentum decay term to 1.
When the momentum decay term is exactly one, no entropy is lost or gained by the optimizer.










How to optimize the variational parameters $\stepsizes$ and $\decays$?
Keep in mind that these could be different for every iteration of SGD as well as every parameter.
Keep in mind that since our approximation to the lower bound is deterministic in $\stepsizes$ and $\decays$, we can easily ``overfit'' the variational hyperparameters unless we average over multiple runs of SGD. [TODO: this doesn't have to be the case. It depends on whether we can reversibly infer the momentum decay rate as we go along.]

\section{Online gradient updates of learning rate and momentum}

A simple procedure to sample from a variational posterior could look like this:
Start with $\stepsize_1 = 1$, $\decay_1 = 1$.
At each iteration of SGD, update each in the direction that would improve the bound at computed at the current step,~$L_{t+1}$:
%
\begin{align}
\vv_{t+1}     & = \decay_t \vv_t - g(\params_t) \\
\params_{t+1} & = \params_t + \stepsize_t \vv_{t+1} \\
\stepsize_{t+1} & = \stepsize_t + s_\stepsize \pderiv{L_t}{\stepsize_t} \\
\decay_{t+1} & = \decay_t + s_\decay \pderiv{L_t}{\decay_t}
\end{align}
%
where $s_\stepsize$ and $s_\decay$ are `meta' step sizes.

Which direction do these gradients point?
For $\stepsize$,
%
\begin{align}
\pderiv{L_t}{\stepsize_t} = g(\params_{t+1}) \vv_{t+1}
\end{align}
%
which means that the learning rate will increase as long as the gradient is pointing in the same direction as the velocity.
It also has the effect of going at the same speed when the gradient becomes small.
This is what we want:  If learning stopped when the gradient was zero, then the optimizer would converge.

For $\decay$, the gradient has the form:
%
\begin{align}
\pderiv{L_t}{\decay_t} = - g(\params_{t+1}) \vv_{t} \stepsize_t - \log \decay_t
\end{align}
%
The first term is exactly the same as our heuristic, scaled by $\stepsize$!
It will increase the velocity if the gradient points in the opposite direction from the velocity.

The second term has a damping effect:  If the gradient, velocity, or stepsize are all small, then this term will try to set the momentum decay term to 1.
When the momentum decay term is exactly one, no entropy is lost or gained by the optimizer.

Taken together, these two update rules mean that when the local gradient goes to zero, the velocity will stabilize and not go to zero, and the stepsize will also remain constant.
This is also necessary for the optimizer not to stop at local minima.
These dynamics will also skate over basins at a constant speed, which is what we want.

[TODO] see if there are any other steady states of this system, such as going in a circle around the minimum of a parabola.

No burn in!

\section{Reversibility}

We can probably play with the order of the SGD steps to make them reversible.
But do we need to?
If we interpret $\stepsizes$ and $\decays$ as just variational parameters, then they are "outside" the system, and the only way they can go wrong is by failing to find a good bound, or by messing up our estimates of the bound.

\section{Alternative approach - decay rate from velocity variance estimate}

If we treat $\decays$ as variational parameters we have to use the same
$\decays$ regardless of starting position and trajectory. Some smoothing will
therefore be necessary to avoid overfitting to a perticular trajectory. If we
evaluate multiple trajectories from an ensemble of starting positions, we can
chose $\decay$ based on the statistical properties of this ensemble.

In particular, we happen to know the target distribution's marginal distribution
over $\vv$ (it's just $N(0, 1)$ independent of $\params$).  The purpose of the
decay term is to collapse the velocity towards this distribution.  We can
estimate the variance, $\sigma_v^2$, of the variational distribution over $\vv$
at time $T$ using the ensemble and then, for example, choose:
\begin{align}
\log \decay = - \beta_0 \log \sigma_v
\end{align}
For some overall ``decay speed'' $\beta_0$. [TODO: think about the multivariate
case, in which $\decays$ could be a matrix. With $N << D$ samples, we can't
hope to estimate the full covariance matrix. Maybe some ones on the diagonal
would help.]

\section{Alternative approach - decay rate adapted using curvature information}

If we base $\decays$ entirely on $\params$ and not on $\vv$ we can have
different $\decays$ for different trajectories and we don't have to worry about
overfitting.

If we have a good estimate of the entropy difference between initial and target
distributions we can choose some integrable $\decays$ schedule that amortizes
the entropy change over multiple steps. The log determinant of the Hessian is
such an estimate. It's exact in the Gaussian case and hopefully a reasonable
approximation otherwise. Looking at the individual eigenvalues of the Hessian
gives a way to estimate the required entropy injection/extraction along each
eigendirection.

[TODO: think about how to avoid actually diagonalizing the Hessian]

[TODO: think about how to handle negative eigenvalues]

\section{Yet another approach - velocity randomization}

An orthonormal transformation of the velocity vector leaves the target
distribution invariant and the entropy unchanged. So if we just apply a
transformation of $I + \alpha T$ at each iteration, we will get a similar effect
to that of a per-parameter $\decay$, without having to devise an elaborate
heuristic to assign a $\decay$ to each parameter. There is just one global
$\decay$ which controls the overall rate of entropy extraction and the remaining
entropy is automatically distributed among the parameters as needed by the
random rotations.

This is a lot like randomizing velocities in the manner of HMC, except that it
doesn't add or remove any net entropy. Entropy is only added or removed when we
apply $\decay$ for which we are rewarded or penalized as appropriate.

------

Since we can sample exactly from $q$, we can approximate the first (``energy'') term of $L$ using one or more Monte Carlo samples.
We can compute the second (``entropy'') term exactly:
\begin{align}
S[q_t] = S[q_0] - \sum_{t=1}^{T} \sum_{d=1}^{D} \log \decay_{td}
\end{align}
%
where $f(\params)$ is the negative log-likelihood and $\decay_{td}$ is the velocity decay rate for step $t$ and parameter $d$.
[Maybe: mention that it's only a lower bound on the entropy in the case of finite precision.]
