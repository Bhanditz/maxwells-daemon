% Sample LaTeX file for creating a paper in the Morgan Kaufmannn two
% column, 8 1/2 by 11 inch proceedings format.

\documentclass[]{article}
\usepackage{uai2015stylefiles/proceed2e}

% Set the typeface to Times Roman
\usepackage{times}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{algpseudocode}
\usepackage{hyperref}


\newcommand{\vx}{\mathbf{x}}
\newcommand{\vX}{\mathbf{X}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vg}{\mathbf{g}}
\newcommand{\vzero}{\mathbf{0}}
\newcommand{\ones}[1]{\mat{1}_{#1}}
\newcommand{\eye}[1]{\mat{E}_{#1}}
\newcommand{\tra}{^{\mathsf{T}}}
\newcommand{\vect}[1]{{\bf{#1}}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\npderiv}[2]{\nicefrac{\partial #1}{\partial #2}}

\newcommand{\distinit}{q_0(\params, \vv)}
\newcommand{\data}{\vx}
\newcommand{\params}{\vx}
\newcommand{\paramsrv}{\vX}  % Random variable.
\newcommand{\numsteps}{T}
\newcommand{\decay}{\beta}
\newcommand{\decays}{{\boldsymbol{\decay}}}
\newcommand{\stepsize}{\alpha}
\newcommand{\stepsizes}{{\boldsymbol{\stepsize}}}


%\title{Maxwell's D\ae mon: Stochastic Gradient Nonparametric Variational Inference}
\title{Entropic Descent: Stochastic Gradient Nonparametric Variational Inference}

\author{} % LEAVE BLANK FOR ORIGINAL SUBMISSION.
          % UAI  reviewing is double-blind.

% The author names and affiliations should appear only in the accepted paper.
%
%\author{ {\bf Harry Q.~Bovik\thanks{Footnote for author to give an
%alternate address.}} \\
%Computer Science Dept. \\
%Cranberry University\\
%Pittsburgh, PA 15213 \\
%\And
%{\bf Coauthor}  \\
%Affiliation          \\
%Address \\
%\And
%{\bf Coauthor}   \\
%Affiliation \\
%Address    \\
%(if needed)\\
%}

\begin{document}

\maketitle

\begin{abstract}
We show how to turn stochastic gradient descent into a procedure that generates a sample from a variational approximate posterior.
The nonparametric variational distribution is implicitly defined as the output of stochastic gradient descent given a random starting point.
The tunable variational parameters are the learning rates and momentum schedules of the optimizer.
We adapt these parameters as the optimizer runs to maximize an approximate variational lower bound on the marginal likelihood.
Entropy is removed from parameters that can effect the likelihood, and injected into parameters to which the likelihood is insensitive.
\end{abstract}


\section{Introduction}

[TODO: Talk about the bridging the gap paper]

\subsection{Contributions}

\begin{itemize}
\item We give a simple algorithm based on stochastic gradient descent that produces samples from a variational posterior along with an estimate of the marginal likelihood.
This algorithm, called entropic descent, performs approximate Bayesian inference in a manner analogous to Hamiltonian Monte Carlo, but without the need for accept-reject steps, and with the ability to operate on minibatches.
This allows regularization parameters to be set within a single training pass.
\item We show that the entropy of our nonparametric variational distribution can be computed exactly by a simple sum of log-momentum decay terms.
\item Early stopping can be viewed as taking a single sample from the implicitly-defined variational distribution.
Tracking the approximate marginal likelihood gives a stopping rule without a validation set.
\item possible contribution:  We derive a variational inference algorithm based on RMSProp, and a corresponding variant of Hamiltonian Monte Carlo.
\end{itemize}

\section{Formulation}

We're trying to maximize the variational lower bound on the marginal likelihood of a model:
%
\begin{align}
L & = \int q(\params) \log p(\params) d\params - \int q(\params) \log q(\params) d\params \\
  & = \mathbb{E}_q[\log p(\params)] + H[q]
\end{align}
%
where $q(\params)$ is a variational distribution.
The two terms can be interpreted as (negative) energy and entropy terms respectively.
We define a nonparametric variational distribution implicitly as the output of a random initialization run through $T$ iterations of stochastic gradient descent (SGD):
%
\begin{align}
\params_1, \vv_1 & \sim \distinit \\
q^{SGD}_T(\params, \vv) & =    SGD(\stepsizes, \decays, \params_1, \vv_1, T)
\end{align}
%

[TODO: specify ``SGD'' function and auxiliary velocity $\vv$.]
Since we can sample exactly from $q$, we can approximate the first (``energy'') term of $L$ using one or more Monte Carlo samples. We can compute the second (``entropy'') term exactly:
\begin{align}
H[q_t] = H[q_0] - \sum_{t=1}^{T} \sum_{d=1}^{D} \log \decay_{td}
\end{align}
%
where $f(\params)$ is the negative log-likelihood and $\decay_{td}$ is the velocity decay rate for step $t$ and parameter $d$.
[Maybe: mention that it's only a lower bound on the entropy in the case of finite precision.]

The variational parameters of $q^{SGD}$ are the learning rate schedule $\stepsizes$, the momentum decay schedule $\decays$ and the initial distribution $\distinit$.
Perhaps for now we'll set $\distinit$ to be the prior, although it could really be anything.

How to optimize the variational parameters $\stepsizes$ and $\decays$?
Keep in mind that these could be different for every iteration of SGD as well as every parameter.
Keep in mind that since our approximation to the lower bound is deterministic in $\stepsizes$ and $\decays$, we can easily ``overfit'' the variational hyperparameters unless we average over multiple runs of SGD. [TODO: this doesn't have to be the case. It depends on whether we can reversibly infer the momentum decay rate as we go along.]

\section{Online gradient updates of learning rate and momentum}

A simple procedure to sample from a variational posterior could look like this:
Start with $\stepsize_1 = 1$, $\decay_1 = 1$.
At each iteration of SGD, update each in the direction that would improve the bound at computed at the current step,~$L_{t+1}$:
%
\begin{align}
\vv_{t+1}     & = \decay_t \vv_t - g(\params_t) \\
\params_{t+1} & = \params_t + \stepsize_t \vv_{t+1} \\
\stepsize_{t+1} & = \stepsize_t + s_\stepsize \pderiv{L_t}{\stepsize_t} \\
\decay_{t+1} & = \decay_t + s_\decay \pderiv{L_t}{\decay_t}
\end{align}
%
where $s_\stepsize$ and $s_\decay$ are `meta' step sizes.

Which direction do these gradients point?
For $\stepsize$,
%
\begin{align}
\pderiv{L_t}{\stepsize_t} = g(\params_{t+1}) \vv_{t+1}
\end{align}
%
which means that the learning rate will increase as long as the gradient is pointing in the same direction as the velocity.
It also has the effect of going at the same speed when the gradient becomes small.
This is what we want:  If learning stopped when the gradient was zero, then the optimizer would converge.

For $\decay$, the gradient has the form:
%
\begin{align}
\pderiv{L_t}{\decay_t} = - g(\params_{t+1}) \vv_{t} \stepsize_t - \log \decay_t
\end{align}
%
The first term is exactly the same as our heuristic, scaled by $\stepsize$!
It will increase the velocity if the gradient points in the opposite direction from the velocity.

The second term has a damping effect:  If the gradient, velocity, or stepsize are all small, then this term will try to set the momentum decay term to 1.
When the momentum decay term is exactly one, no entropy is lost or gained by the optimizer.

Taken together, these two update rules mean that when the local gradient goes to zero, the velocity will stabilize and not go to zero, and the stepsize will also remain constant.
This is also necessary for the optimizer not to stop at local minima.
These dynamics will also skate over basins at a constant speed, which is what we want.

[TODO] see if there are any other steady states of this system, such as going in a circle around the minimum of a parabola.


\section{Reversibility}

We can probably play with the order of the SGD steps to make them reversible.
But do we need to?
If we interpret $\stepsizes$ and $\decays$ as just variational parameters, then they are "outside" the system, and the only way they can go wrong is by failing to find a good bound, or by messing up our estimates of the bound.

\section{Alternative approach - decay rate from velocity variance estimate}

If we treat $\decays$ as variational parameters we have to use the same
$\decays$ regardless of starting position and trajectory. Some smoothing will
therefore be necessary to avoid overfitting to a perticular trajectory. If we
evaluate multiple trajectories from an ensemble of starting positions, we can
chose $\decay$ based on the statistical properties of this ensemble.

In particular, we happen to know the target distribution's marginal distribution
over $\vv$ (it's just $N(0, 1)$ independent of $\params$).  The purpose of the
decay term is to collapse the velocity towards this distribution.  We can
estimate the variance, $\sigma_v^2$, of the variational distribution over $\vv$
at time $T$ using the ensemble and then, for example, choose:
\begin{align}
\log \decay = - \beta_0 \log \sigma_v
\end{align}
For some overall ``decay speed'' $\beta_0$. [TODO: think about the multivariate
case, in which $\decays$ could be a matrix. With $N << D$ samples, we can't
hope to estimate the full covariance matrix. Maybe some ones on the diagonal
would help.]

\section{Alternative approach - decay rate adapted using curvature information}

If we base $\decays$ entirely on $\params$ and not on $\vv$ we can have
different $\decays$ for different trajectories and we don't have to worry about
overfitting.

If we have a good estimate of the entropy difference between initial and target
distributions we can choose some integrable $\decays$ schedule that amortizes
the entropy change over multiple steps. The log determinant of the Hessian is
such an estimate. It's exact in the Gaussian case and hopefully a reasonable
approximation otherwise. Looking at the individual eigenvalues of the Hessian
gives a way to estimate the required entropy injection/extraction along each
eigendirection.

[TODO: think about how to avoid actually diagonalizing the Hessian]

[TODO: think about how to handle negative eigenvalues]

\section{Yet another approach - velocity randomization}

An orthonormal transformation of the velocity vector leaves the target
distribution invariant and the entropy unchanged. So if we just apply a
transformation of $I + \alpha T$ at each iteration, we will get a similar effect
to that of a per-parameter $\decay$, without having to devise an elaborate
heuristic to assign a $\decay$ to each parameter. There is just one global
$\decay$ which controls the overall rate of entropy extraction and the remaining
entropy is automatically distributed among the parameters as needed by the
random rotations.

This is a lot like randomizing velocities in the manner of HMC, except that it
doesn't add or remove any net entropy. Entropy is only added or removed when we
apply $\decay$ for which we are rewarded or penalized as appropriate.

\end{document}
